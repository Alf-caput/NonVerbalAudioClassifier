{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alf-caput/NonVerbalAudioClassifier/blob/main/src/tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ATPf6j4cLRgv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "zip_filename = 'vs_release_16k.zip'\n",
        "\n",
        "if not os.path.exists(zip_filename):\n",
        "    os.system(\"wget https://www.dropbox.com/s/fuld3z222j9t1oy/vs_release_16k.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR7UfICDLRgx"
      },
      "source": [
        "Dentro de vs_release_16k.zip descomprimimos:  \n",
        "- audio_16k  \n",
        "\n",
        "Nos servimos del módulo zipfile para descomprimir.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vyOyxxYdLRgy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "zip_filename = 'vs_release_16k.zip'\n",
        "audio_dir = 'audio_16k/'\n",
        "\n",
        "if not os.path.exists(audio_dir):\n",
        "    with ZipFile(zip_filename, 'r') as zf:\n",
        "        with ThreadPoolExecutor() as exe:\n",
        "            for file in zf.namelist():\n",
        "                if file.startswith(audio_dir):\n",
        "                    exe.submit(zf.extract, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total audios: 20502\n",
            "Filenames shape: TensorSpec(shape=(), dtype=tf.string, name=None)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "audio_dir = 'audio_16k/'\n",
        "seed = 42\n",
        "\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "ds_files = tf.data.Dataset.list_files(audio_dir + '*.wav') # by default shuffle=True\n",
        "\n",
        "print(\"Total audios:\", len(ds_files))\n",
        "print(\"Filenames shape:\", ds_files.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'audio_16k\\\\m1788_0_sneeze.wav', shape=(), dtype=string)\n",
            "tf.Tensor(b'audio_16k\\\\m1594_0_laughter.wav', shape=(), dtype=string)\n",
            "tf.Tensor(b'audio_16k\\\\m2453_0_cough.wav', shape=(), dtype=string)\n",
            "tf.Tensor(b'audio_16k\\\\m2737_0_sigh.wav', shape=(), dtype=string)\n",
            "tf.Tensor(b'audio_16k\\\\m0801_0_sneeze.wav', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for elem in ds_files.take(5):\n",
        "    print(elem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41hDfCPBb4iS",
        "outputId": "363ddb11-b822-44a2-f16f-c86f5ad803fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename Tensor: tf.Tensor(b'audio_16k\\\\o2139_0_sigh.wav', shape=(), dtype=string)\n",
            "Label Tensor: tf.Tensor(2, shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocal_sounds = ['cough', 'laughter', 'sigh', 'sneeze', 'sniff', 'throatclearing']\n",
        "num_sounds = len(vocal_sounds)\n",
        "\n",
        "for elem in ds_files.take(1):\n",
        "    example_file = elem\n",
        "\n",
        "def get_label(file_path):\n",
        "  label = tf.strings.split(\n",
        "    input=file_path,\n",
        "    sep='_')[-1]\n",
        "\n",
        "  label = tf.strings.split(\n",
        "    input=label,\n",
        "    sep='.')[0]\n",
        "\n",
        "  label = tf.where(label==vocal_sounds)\n",
        "  label = tf.reshape(label, ())\n",
        "  return label\n",
        "\n",
        "print(\"Filename Tensor:\", example_file)\n",
        "print(\"Label Tensor:\", get_label(example_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt7HzlPLo6gf",
        "outputId": "3fc65a71-732a-4bf1-d3a4-711383c0ba39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename Tensor: tf.Tensor(b'audio_16k\\\\m0131_0_cough.wav', shape=(), dtype=string)\n",
            "Signal Tensor: tf.Tensor(\n",
            "[3.0517578e-05 3.0517578e-05 0.0000000e+00 ... 4.2724609e-04 4.5776367e-04\n",
            " 3.3569336e-04], shape=(36864,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "std_len = 64000\n",
        "\n",
        "for elem in ds_files.take(1):\n",
        "    example_file = elem\n",
        "\n",
        "def get_signal(file_path):\n",
        "  \n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  signal = tf.audio.decode_wav(audio_binary)[0]\n",
        "  \n",
        "  # curr_len = tf.size(signal)\n",
        "  \n",
        "  # if curr_len < std_len:\n",
        "  #   signal = tf.pad(signal, [[0, std_len-curr_len], [0, 0]], \"SYMMETRIC\")\n",
        "    \n",
        "  # elif curr_len > std_len:\n",
        "  #   rand_idx = tf.random.uniform(shape=[], maxval=curr_len - std_len, dtype=tf.int32)\n",
        "  #   signal = signal[rand_idx:]\n",
        "\n",
        "  # else:\n",
        "  #   pass\n",
        "  \n",
        "  return tf.squeeze(signal, axis=-1)\n",
        "\n",
        "print(\"Filename Tensor:\", example_file)\n",
        "print(\"Signal Tensor:\", get_signal(example_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQMUH4QNbZ3P",
        "outputId": "a0c1a401-da04-46b9-950b-6bff369885c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total signals: 20502\n",
            "Signals shape: (TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
          ]
        }
      ],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "ds_signals = (\n",
        "    ds_files\n",
        "    .shuffle(len(ds_files))\n",
        "    .cache()\n",
        "    .map(lambda x: (get_signal(x), get_label(x)), num_parallel_calls=AUTOTUNE)\n",
        "    )\n",
        "\n",
        "print(\"Total signals:\", len(ds_signals))\n",
        "print(\"Signals shape:\", ds_signals.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "tpy5OW7Fs1Ek",
        "outputId": "f1e88231-45ec-4fee-fee8-6fc48931b501"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for elem in ds_signals.take(1):\n",
        "  signal, label = elem\n",
        "  plt.plot(signal)\n",
        "  plt.title(vocal_sounds[label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvcOpKxRtE3d",
        "outputId": "88789cea-8629-406b-85a1-eccccf96dad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total audios: 20502\n"
          ]
        }
      ],
      "source": [
        "def remove_shortaudio(file_path, min_len):\n",
        "  if get_signal(file_path).shape[0] < min_len:\n",
        "    tf.io.gfile.remove(file_path)\n",
        "\n",
        "initial_file_count = 21024\n",
        "\n",
        "if len(ds_files) >= initial_file_count:\n",
        "  # Remove less than 0.1s audios (16kHz then 1600 frames)\n",
        "  min_len = 1600\n",
        "  with ThreadPoolExecutor() as exe:\n",
        "    for file in ds_files:\n",
        "      exe.submit(remove_shortaudio, file, min_len)\n",
        "\n",
        "  ds_files = tf.data.Dataset.list_files(audio_dir + '*.wav') # by default shuffle=True\n",
        "\n",
        "print('Total audios:', len(ds_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9yA9_bQlv1M",
        "outputId": "f7822268-7a4c-40c8-b1d1-f2e41149c4ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(62805,), dtype=float32, numpy=\n",
            "array([ 0.00720215,  0.01242065,  0.01254272, ..., -0.00039673,\n",
            "        0.00570679,  0.00408936], dtype=float32)>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n"
          ]
        }
      ],
      "source": [
        "ds_signals = ds_files.map(lambda x: (get_signal(x), get_label(x)), num_parallel_calls=AUTOTUNE)\n",
        "for elem in ds_signals.take(1):\n",
        "  print(elem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "3dST_H_VmB_A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(), dtype=int32) Tensor(\"strided_slice_1:0\", shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "vocal_sounds = ['cough', 'laughter', 'sigh', 'sneeze', 'sniff', 'throatclearing']\n",
        "num_sounds = len(vocal_sounds)\n",
        "\n",
        "\n",
        "def normalize(signal, label):\n",
        "  # Label to onehot\n",
        "  label = tf.one_hot(label, len(vocal_sounds))\n",
        "\n",
        "  # Signal to standard length\n",
        "  std_len = 64_000 # 4s\n",
        "  curr_len = len(signal)\n",
        "  print(curr_len, len(signal))\n",
        "\n",
        "  if curr_len > std_len:\n",
        "    rand_idx = tf.random.uniform(shape=[], maxval=curr_len-std_len, dtype=tf.int32)\n",
        "    signal = signal[rand_idx:rand_idx+std_len]\n",
        "\n",
        "  else:\n",
        "    padding_len = std_len - curr_len\n",
        "    signal = tf.pad(signal, [[0, padding_len]], \"SYMMETRIC\")\n",
        "\n",
        "  return signal, label\n",
        "\n",
        "ds_signals = ds_signals.map(normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYTpsGb3MtS_",
        "outputId": "f5567a54-2d55-4543-ccf5-e804636a3bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(64000,), dtype=float32, numpy=\n",
            "array([ 5.1574707e-03,  3.4484863e-03, -8.1787109e-03, ...,\n",
            "        0.0000000e+00,  3.0517578e-05,  0.0000000e+00], dtype=float32)>, <tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 1.], dtype=float32)>)\n"
          ]
        }
      ],
      "source": [
        "for elem in ds_signals.take(1):\n",
        "  print(elem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MntLIWZJcFdb",
        "outputId": "7306e65a-3fe3-4792-9231-7082a97113b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None,), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(6,), dtype=tf.float32, name=None))"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_signals.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AtLbAh6jLRg8"
      },
      "outputs": [],
      "source": [
        "num_train = len(ds_signals)\n",
        "\n",
        "RATIO = 0.2\n",
        "test_size = int(RATIO * num_train)\n",
        "\n",
        "ds_train = ds_signals.skip(test_size)\n",
        "ds_test = ds_signals.take(test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IwiMw9keLRg8"
      },
      "outputs": [],
      "source": [
        "num_train = len(ds_train)\n",
        "\n",
        "RATIO = 0.2\n",
        "validation_size = int(RATIO * num_train)\n",
        "\n",
        "ds_fit = ds_signals.skip(validation_size)\n",
        "ds_val = ds_signals.take(validation_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fkTw9s3lLRg9"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 1024\n",
        "ds_fit = (\n",
        "    ds_fit\n",
        "    .cache()\n",
        "    .shuffle(len(ds_fit))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "ds_val = (\n",
        "    ds_val\n",
        "    .cache()\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cq9rBe_GLRg9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Reduce learning rate\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=4, min_lr=0.0001)\n",
        "\n",
        "epoch_wait = 5\n",
        "# Crear el EarlyStopping callback con la función on_train_end\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=epoch_wait, restore_best_weights=True)\n",
        "\n",
        "# Definimos los callbacks\n",
        "callbacks = [early_stopping, reduce_lr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZBl67bkdLRg-"
      },
      "outputs": [],
      "source": [
        "# Establecer la semilla global\n",
        "seed_value = 42\n",
        "tf.random.set_seed(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_jh0jR1LRg-",
        "outputId": "178a628e-f268-4378-ebd7-b7fcf021f300"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096,064</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m4,096,064\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m198\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,098,342</span> (15.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,098,342\u001b[0m (15.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,098,342</span> (15.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,098,342\u001b[0m (15.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Arquitectura de la red convolucional\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(64000,)),  # Capa de entrada con 64 neuronas\n",
        "    Dense(32, activation='relu'),  # Capa oculta con 32 neuronas\n",
        "    Dense(6, activation='sigmoid')  # Capa de salida con una neurona para clasificación binaria\n",
        "])\n",
        "\n",
        "# Compila el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Resumen de la arquitectura del modelo\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__X9T6ZQLRg-",
        "outputId": "4f5d4860-dcf2-4b8b-9e59-cd9f7521d44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:329\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    328\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 329\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    331\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    332\u001b[0m     )\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[0;32m    916\u001b[0m           bound_args\n\u001b[0;32m    917\u001b[0m       )\n\u001b[0;32m    918\u001b[0m   )\n\u001b[1;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[0;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py:284\u001b[0m, in \u001b[0;36mInvalidArgumentError.__init__\u001b[1;34m(self, node_def, op, message, *args)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors.InvalidArgumentError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidArgumentError\u001b[39;00m(OpError):\n\u001b[0;32m    272\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Raised when an operation receives an invalid argument.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m  This error is typically raised when an op receives mismatched arguments.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m  InvalidArgumentError: ...\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_def, op, message, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    285\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates an `InvalidArgumentError`.\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28msuper\u001b[39m(InvalidArgumentError, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(node_def, op, message,\n\u001b[0;32m    287\u001b[0m                                                INVALID_ARGUMENT, \u001b[38;5;241m*\u001b[39margs)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
            "\u001b[0mNormalization failed: type=InvalidArgumentError args=(None, None, '{{function_node __inference_one_step_on_iterator_2291}} Cannot batch tensors with different shapes in component 0. First element had shape [51883] and element 599 had shape [35499].\\n\\t [[{{node IteratorGetNext}}]] [Op:__inference_one_step_on_iterator_2291]', {})"
          ]
        }
      ],
      "source": [
        "history = model.fit(ds_fit, epochs=100, validation_data=ds_val, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkUnbrUSLRg_",
        "outputId": "133774e4-e07e-4299-bfb2-2ada5256a7b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">65,537,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │    \u001b[38;5;34m65,537,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │         \u001b[38;5;34m3,078\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,327,558</span> (253.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,327,558\u001b[0m (253.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,327,558</span> (253.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,327,558\u001b[0m (253.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.1627 - loss: 0.5726 - val_accuracy: 0.1742 - val_loss: 0.5107 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.5234 - loss: 0.4107 - val_accuracy: 0.2560 - val_loss: 0.6486 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m16/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.8415 - loss: 0.1717"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Resumen de la arquitectura del modelo\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:329\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    328\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 329\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    331\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    332\u001b[0m     )\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\ProyectoIntegradorIII\\NonVerbalAudioClassifier\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Arquitectura de la red convolucional\n",
        "model = Sequential([\n",
        "    Dense(1024, activation='relu', input_shape=(64000,)),  # Capa de entrada con 1024 neuronas\n",
        "    Dense(512, activation='relu'),  # Capa oculta con 512 neuronas\n",
        "    Dense(512, activation='relu'),  # Capa oculta con 512 neuronas\n",
        "    Dense(6, activation='sigmoid')  # Capa de salida con una neurona para clasificación binaria\n",
        "])\n",
        "\n",
        "# Compila el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Resumen de la arquitectura del modelo\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(ds_fit, epochs=100, validation_data=ds_val, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0r_yhAALRg_",
        "outputId": "87dd4531-e635-4261-d8c8-31113fdf4ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 1s 22ms/step - loss: 1.1692 - accuracy: 0.3865\n",
            "Test accuracy: 0.3865366280078888\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "ds_eval = (\n",
        "    ds_test.cache()\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "test_loss, test_acc = model.evaluate(ds_eval)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
